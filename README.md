# Tensorflow.Keras-and-CIFAR-10
target 90% val_acc with easy neural network
Основная идея использовать простой в использовании keras и его основные инструменты: MaxPooling2D, BatchNormalization, Dropout, l2_kernel для создания простой нейронной сети, которая пробьет барьер в 90% Val_acc. 
Также опробывать для оптимизации гиперпараметров keras_tuner

# Папки и файлы
## Model - включает в себя основные модели до финальной
 1. First_model.py - Чуть улучшенная модель из документации keras. Val-acc ~75%
 2. Second_model.py - Больше Dropout и BatchNormalization после каждого слоя. Параметры подобраны с помощью тюнера.
 3. model_step_3.py - Чуть лучшие показатели дает функция активации 'elu', поэтому поставил ее. ~82% val_acc
 4. model_step_4.py - Углубил сеть, поставил стандартное число нейронов ( не из тюнера). Добавил l2_regularizer, который по итогу не дал особого эффекта. Модель и без него показывает результаты ~85-86% val_acc
 ## Keras_tuner - использование тюнера для оптимизации гиперпараметров
 1. tuner_step_1.py - оптимизация на количество слоев и нейронов. Было несколько совершенно разных результатов по слоям, но с одинаковой точностью. 
 2. tuner_step_2.py - оптимизация функции активации. Плохой результат только у tanh, что неудивительно, так как его результаты от [-1,1]
 3. tuner_final.py - вариант для оптимизации финальной модели, если нужно выжать еще больше точность
 
 ## Functional API keras - попытки выстроить не Sequential model c помощью функционального API keras.
 try_1.py, try_2.py - пример 2-х попыток. Преодолеть потолок в 90% с помощью них не удалось
 
 ## final_model.py 
 Финальная модель, в которой слоев еще больше, но это не сильно повлияло на скорость. Изменена функции оптимизации с adam на RTS и предварительно делается небольшая аугментация фото, затем для добивки прогоняется на обычных фото. Финальный val_acc 90-91%
 
 Интересный момент, что более сильная аугментация показывала худшие результаты, чем простенькая.
